全面研究英文分词（Tokenization）方法：从经典算法到前沿进展1. 引言分词（Tokenization）是将原始文本序列分解为更小、更易于管理单元（称为“词符”或“token”）的过程，这些单元可以是单词、字符、子词，甚至是句子 1。在自然语言处理（NLP）领域，分词是几乎所有后续任务（如词性标注、命名实体识别、情感分析、机器翻译和大规模语言模型操作）不可或缺的预处理步骤 1。若无分词，人工智能系统将无法识别单词，句子结构将丢失，文本处理亦无从谈起 9。例如，系统会将“theemailisunread”视为一个无意义的字符串，而不是“the email is unread” 9。尽管分词至关重要，但它也面临诸多挑战。这些挑战包括处理缩略词、复合词、没有明确词边界的语言（如中文，尽管本报告聚焦英文分词）、词义的模糊性（如“bank”可指金融机构或河岸）以及对上下文的依赖 1。此外，分词通常是语言相关的，不同语言可能需要不同的技术 10。罕见词或未登录词（Out-of-Vocabulary, OOV）的处理、分词引入的偏见、以及多语言环境下的复杂性，都对分词算法提出了严峻考验 9。分词作为NLP流程中的初始环节，其质量直接影响整个系统的性能。不恰当的分词可能导致信息丢失或歪曲，从而限制后续模型理解和生成文本的能力 5。因此，分词不仅仅是一个简单的文本分割步骤，它更像是一个基础性的瓶颈，其设计和实现的选择会对模型的效率、成本、公平性乃至最终的应用效果产生深远影响 7。这一过程本身也是一种对复杂连续语言信号的必要简化，通过将文本转化为离散的词符序列，使得计算机能够以结构化的方式处理和理解人类语言 1。本报告旨在全面研究英文分词方法，涵盖从经典的基于规则的算法到现代大规模语言模型中广泛采用的基于子词的先进算法。研究将参考 nlpprogress.com 和 arXiv 等来源的最新进展，并包含详细的 LaTeX 数学公式推导和 Python 算法实现代码，以期为相关领域的研究人员和实践者提供一个系统性的视角。2. 基于规则的分词方法基于规则的分词方法是最早期的技术之一，依赖预定义的语言学规则来切分文本。这些规则通常基于空格、标点符号或更复杂的模式（如正则表达式）2。2.1. 简单的基于规则的方法2.1.1. 空白、标点和基本分割符分词最基础的规则分词方法是基于空白字符（如空格、制表符、换行符）和标点符号进行文本切分。空白分词 (Whitespace Tokenization): 此方法根据空白字符将文本分割成词符 2。例如，句子“This is a sample text.”将被分割为。Python 实现示例 2:Python# 步骤 1: 加载输入文本
text = "The quick brown fox jumps over the lazy dog."
# 步骤 2: 定义分词规则 (基于空白分割)
tokens = text.split()
# 步骤 4: 输出词符
print(tokens)
# 输出:
标点分词 (Punctuation Tokenization): 此方法根据标点符号（如句号、逗号、分号）将文本分割成词符 2。通常与空白分词结合使用，将标点符号视为独立的词符。例如，句子“Hello Geeks! How can I help you?”可以被分割为 ['Hello', 'Geeks', '!', 'How', 'can', 'I', 'help', 'you', '?']。Python 实现示例 2:Pythonimport re
text = "Hello Geeks! How can I help you?"
# 匹配单词字符序列或任何非单词、非空白的单个字符（即标点）
tokens = re.findall(r'\b\w+\b|[^\w\s]', text)
print(tokens)
# 输出: ['Hello', 'Geeks', '!', 'How', 'can', 'I', 'help', 'you', '?']
这些简单方法实现起来直接且处理速度快 3。然而，它们的局限性也十分明显：难以妥善处理未登录词（OOV words）、对语言特性（如德语的复合词 2）敏感，并且无法捕捉词内信息（即子词信息）3。此外，若无额外规则，它们在处理缩略词（如 "don't"）或连字符词（如 "state-of-the-art"）时也可能表现不佳。2.1.2. 正则表达式分词器正则表达式分词器利用正则表达式来定义词符边界的模式 2。这种方法提供了比简单分割符更大的灵活性和控制力，尤其适用于提取文本中特定模式的字符串，如电子邮件地址、电话号码、货币金额等 2。Python 实现示例 2:Pythonimport re
text = "Contact us at info@example.com or support@example.org."
email_pattern = r'[\w\.-]+@[\w\.-]+\.\w+'
emails = re.findall(email_pattern, text)
print(emails)
# 输出: ['info@example.com', 'support@example.org']
基于规则的系统，无论是简单的空白/标点分割还是复杂的正则表达式，都体现了一种在简易性/控制性与普适性/鲁棒性之间的权衡。它们允许开发者对特定情况（如特定格式的日期或专有名词）进行精确控制和明确定义处理方式 2。这种明确性使得分词结果易于理解和调试。然而，这种优势的另一面是其固有的脆弱性。为英语（更不用说其他语言）中所有语言现象制定详尽无遗的规则是一项艰巨且往往不稳定的任务。随着语言的演变或遇到新的文本领域，这些规则需要持续的维护和更新。其高度依赖特定语言的特性 ，限制了它们在未经修改的情况下跨多种文本类型的适用性。正是这种内在局限性，为数据驱动的分词方法的兴起铺平了道路 13。2.2. 宾州树库 (Penn Treebank, PTB) 分词标准宾州树库（PTB）分词标准是英语分词领域一个被广泛参考的标准，常被用作基准或其它分词器的基础。它通常假设文本已预先分割为句子，然后利用正则表达式进行分词 14。PTB 分词器的核心规则包括 14：处理缩略词： 分割常见的英文缩略词，例如 "don't" 分词为 "do n't"，"they'll" 分词为 "they 'll" 14。处理标点符号： 将标点符号作为独立的词符处理 14。逗号和单引号： 当逗号和单引号后跟有空白时，将它们从单词中分离出来 14。句末句号： 分离出现在句子末尾的句号 14。括号标准化： 将圆括号标准化为 "-LRB-" 和 "-RRB-"，其他常见括号（如花括号、方括号）也进行类似处理，映射为 "-LCB-"、"-RCB-" 等 15。引号处理： 提供多种选项处理引号，包括 ASCII 引号（' 和 "）、LaTeX 引号（``, `, ', ''）以及 Unicode 引号（U+2018 至 U+201D）15。默认情况下，倾向于使用 LaTeX 风格的引号。省略号和破折号： 根据 PTB 的传统约定处理省略号（通常映射为三个点 "..."）和各种破折号（通常映射为 "--"）15。拼写美式化 (可选)： 可选择将常见的英式英语拼写改写为美式英语拼写，这对于训练材料使用美式英语（如 PTB 本身）的场景非常有用 15。处理不可分词字符： 对分词器无法识别的字符提供处理选项，如删除或保留为单个字符词符，并可选择是否记录警告 15。PTB 分词标准的出现提供了一种标准化的方法，促进了 NLP 研究中结果的可比性。尽管它的一些约定（如用 "-LRB-" 表示左括号）在现代应用中可能显得有些过时，但对于理解历史背景以及与基于 PTB 训练的模型的兼容性而言，这些约定仍然具有重要意义。诸如 PTB 这样一套详尽规则标准的存在，本身就凸显了一个事实：构成一个“词符”的定义并非总是显而易见的 14。如果分词简单到仅靠空格分割就能完成，那么这些精心制定的规则便无存在的必要。PTB 对缩略词、各类标点符号乃至不同类型引号的细致处理，揭示了在一致性地定义词边界时所涉及的大量边缘情况和决策点。这种复杂性直接反映了自然语言固有的“混乱”和多样性，也解释了为何分词本身就是一个具有挑战性的研究课题 10。3. 子词分词：原理与基础算法随着深度学习模型在NLP领域的广泛应用，传统词级别分词的局限性日益突出，催生了子词（subword）分词技术的发展。3.1. 子词单元的需求：平衡词汇量与表示能力词级别分词的局限性：词汇量爆炸： 对于包含大量独特词汇的语料库，词级别分词会导致非常庞大的词汇表 9。这不仅增加了模型的参数量和内存消耗，也使得模型训练更加困难。OOV（未登录词）问题： 无法有效处理词汇表中未出现的词（如罕见词、新词、拼写错误），这些词通常被映射为一个统一的<UNK>（未知）词符，导致重要信息的丢失 3。形态关系缺失： 无法捕捉词与词之间的形态学关系，例如，“read”、“reads”、“reading”会被视为完全不同的、无关联的词符 9。字符级别分词的局限性：词汇量极小： 词汇表仅包含所有单个字符，规模可控。序列过长： 即便简短的句子也会产生非常长的词符序列 1。计算负担重： 长序列显著增加了模型的计算负担，并需要更大的上下文窗口来捕捉依赖关系 9。语义信息稀疏： 单个字符在独立存在时通常携带较少的语义信息，模型需要从极长的字符序列中学习更高层次的语义结构，这增加了学习难度 16。子词分词作为解决方案：子词分词旨在上述两种极端之间取得平衡。它将罕见词分解为更小且有意义的单元（子词或词素），同时保留常用词作为单个词符 1。有效管理词汇量： 通过共享子词单元，可以在不显著增加词汇表大小的情况下表示大量词汇 17。处理OOV词： 未登录词可以通过其组成的已知子词序列来表示，从而更好地保留其语义信息，而不是简单地映射为<UNK> 1。捕捉形态信息： 子词能够自然地表示词的形态变体，例如，“reading” 可能被分解为 “read” 和 “##ing” （其中 “##” 表示该子词是词的一部分而非词首）1。广泛应用： 子词分词已成为现代大规模语言模型（如 BERT、GPT 系列）的标准配置 1。子词分词的出现并非仅仅是在词级别和字符级别之间寻找一个折中点，它更体现了对一个多目标优化问题的求解尝试。这个优化问题涉及以下几个相互竞争的因素：词汇量大小（Vocabulary Size）： 保持词汇量在可管理范围内，以减少模型参数和内存占用。序列长度（Sequence Length）： 使分词后的序列尽可能短，以提高计算效率。表示能力（Representational Power）： 确保词符具有足够的语义意义，并且能够表示任何单词，包括OOV词。语言学合理性（Linguistic Plausibility）： 理想情况下，子词应与语言学上的词素或其他有意义的语言单元对齐。不同的子词分词算法（如BPE、WordPiece、Unigram）代表了解决这一多目标优化问题的不同启发式策略。这些策略通常将分词问题框架化为一个数据压缩问题 19 或一个似然最大化问题 20，试图在这些相互制约的因素间找到最佳平衡点。3.2. 字节对编码 (Byte-Pair Encoding, BPE)BPE 是一种广泛应用于现代NLP模型（如GPT系列、RoBERTa、Llama2 5）的子词分词算法，其根源可追溯到数据压缩领域。3.2.1. 算法原理：从数据压缩到自然语言处理BPE最初由Gage于1994年提出，作为一种数据压缩技术 19。其核心思想是：在训练语料中，迭代地找出最常出现的相邻字节对（在NLP应用中通常是字符对或符号对），并将它们合并成一个新的、单一的符号/词符 19。词汇构建过程：初始化词汇表： 词汇表最初包含语料库中所有的单个字符（或字节，通常是256个基本字节值 23）。文本首先被预分词（例如，按空格分割），并且通常会在每个单词的末尾添加一个特殊的结束符（如</w>）以区分词边界。统计符号对频率： 计算当前语料库（表示为符号序列的集合，每个符号来自当前词汇表）中所有相邻符号对的出现频率 24。找出最频繁符号对： 确定步骤2中频率最高的符号对。合并与更新： 将此最频繁符号对合并成一个新的符号，并将这个新符号添加到词汇表中。同时，更新语料库，将所有出现该符号对的地方替换为这个新合并的符号 24。迭代： 重复步骤2-4，直到达到预定义的合并次数（即期望的词汇表大小）或没有符号对的出现次数大于1 19。文本分词（编码）过程：给定一段新的文本，首先进行与训练时相同的预处理（例如，分割单词，添加结束符）。然后，按照学习到的合并规则的顺序（通常是按学习到的先后顺序或优先级从高到低），贪婪地将文本分割成词汇表中最长的已知子词序列 23。如果遇到词汇表中不存在的字符（这在BPE中较少见，因其通常从所有基础字符/字节开始），可能通过字节级分解或映射到未知词符来处理。3.2.2. 数学表述 (概念性)BPE的词汇构建过程可以概念化如下：令 V 为词汇表，初始时 V0​ 包含所有单个字符/字节。令 C 为训练语料库，表示为一个由来自 V 的符号组成的序列集合。在每次迭代 j=1,…,k（其中 k 是总合并次数）中：找出符号对 $(a_j^$, b_j^)，使得该对在当前语料库 Cj−1​（基于词汇表 Vj−1​ 表示）中的频率 $\text{freq}(a_j^$, b_j^ \mid C_{j-1}, V_{j-1}) 最大。(aj)​=arg(a,b)max​freq(a,b∣Cj−1​,Vj−1​)创建一个新的合并符号 $c_j = \text{merge}(a_j^$, b_j^)。将新符号添加到词汇表中：Vj​=Vj−1​∪cj​。通过将语料库 Cj−1​ 中所有出现的 $(a_j^$, b_j^) 替换为 cj​，得到更新后的语料库 Cj​：$$C_j = \text{replace_all}(C_{j-1}, (a_j^), c_j)$$此过程隐式的目标是实现良好的文本压缩，研究表明压缩效果与分词质量存在相关性 19。BPE 是一种贪婪启发式算法；对于 k>1 次合并，它可能无法达到全局最优的压缩效果 19。事实上，寻找最小语法生成给定文本的问题是NP难的 19，这意味着任何多项式时间算法（如BPE）本质上都是一种近似解。“压缩效用”可以定义为在一次合并操作后，语料库中符号总数的减少量。3.2.3. Python 实现：核心逻辑以下Python代码片段展示了BPE词汇学习和文本编码的核心思想，参考了 23 中的概念。词汇学习 (Vocabulary Learning):Pythonfrom collections import defaultdict, Counter

def get_word_char_freqs(corpus_counts):
    """将词频语料库转换为字符级别表示的词频，并添加</w>"""
    char_freqs = defaultdict(int)
    for word, freq in corpus_counts.items():
        # 为每个词添加词尾符号 </w>，并用空格分隔字符
        # 例如 "low": 5 -> "l o w </w>": 5
        spaced_word = ' '.join(list(word)) + ' </w>'
        char_freqs[spaced_word] = freq
    return char_freqs

def get_pair_stats(vocab_freqs):
    """统计词汇中所有相邻符号对的频率"""
    pairs = defaultdict(int)
    for word_str, freq in vocab_freqs.items():
        symbols = word_str.split()
        for i in range(len(symbols) - 1):
            pairs[(symbols[i], symbols[i+1])] += freq
    return pairs

def merge_pair(pair_to_merge, vocab_freqs_in):
    """在词汇中合并指定的符号对"""
    vocab_freqs_out = defaultdict(int)
    merged_symbol = pair_to_merge + pair_to_merge # 直接连接，不带空格
    # 正则表达式用于安全地替换，确保只替换整个符号
    # 例如，如果pair_to_merge是('a', 'b')，则查找 ' a b ' (注意前后的空格)
    # 并替换为 ' ab ' (新符号前后也有空格，除非它是词首或词尾)
    # 这简化了split()和join()的逻辑
    # 更鲁棒的实现会直接操作符号列表
    # 此处为了简化，我们假设符号不包含空格
    
    # 构建一个正则表达式来匹配这对符号，确保它们是独立的符号（由空格包围）
    # 或者在词的开始/结束位置
    # 例如：best_pair = ('t', '</w>') -> merged_symbol = 't</w>'
    # regex: r'\bt\s+</w>\b' (大致思路)
    # Python的re.sub在循环中对字符串操作可能效率不高，真实BPE实现通常在符号列表上操作

    # 简单的字符串替换方法（可能不完全鲁棒，但易于理解）
    # 我们需要将 'sym1 sym2' 替换为 'sym1sym2'
    str_pair_to_merge = pair_to_merge + ' ' + pair_to_merge
    
    for word_str, freq in vocab_freqs_in.items():
        # 将 'a b' 替换为 'ab'
        # 注意：这种简单的replace可能导致问题，例如如果符号本身包含空格
        # 或部分匹配。一个更健壮的方法是分割成符号列表，执行合并，然后重新连接。
        new_word_str = word_str.replace(str_pair_to_merge, merged_symbol)
        vocab_freqs_out[new_word_str] = freq
        
    return vocab_freqs_out

# 示例语料库 (词及其频率)
corpus_counts = {"low": 5, "lower": 2, "newest": 6, "widest": 3, "hugging": 7}
# 转换为BPE初始格式：{'l o w </w>': 5,...}
current_vocab_freqs = get_word_char_freqs(corpus_counts)

num_merges = 10
learned_merges = # 存储学习到的合并规则 (pair -> merged_symbol)
vocab = set() # 最终的子词词汇表
# 初始化词汇表为所有单个字符
for word_str in current_vocab_freqs:
    for char_token in word_str.split():
        vocab.add(char_token)


print("Initial vocab (chars):", sorted(list(vocab)))
print("Initial corpus representation:", current_vocab_freqs)

for i in range(num_merges):
    pair_stats = get_pair_stats(current_vocab_freqs)
    if not pair_stats:
        print("No more pairs to merge.")
        break
    
    # 找出频率最高的符号对
    # 如果有多个频率相同的对，通常选择在排序中最先出现的那个
    # (Python的max行为是如果值相同，返回第一个遇到的键)
    best_pair = max(pair_stats, key=pair_stats.get)
    
    if pair_stats[best_pair] < 2 and num_merges > len(vocab): # 启发式：如果最高频率低于2，可能停止
        print(f"Highest pair frequency is {pair_stats[best_pair]}. Stopping early.")
        break

    print(f"\nMerge {i+1}: Merging pair {best_pair} with frequency {pair_stats[best_pair]}")
    
    merged_symbol = best_pair + best_pair
    learned_merges.append(best_pair) # 记录合并规则
    vocab.add(merged_symbol) # 将新合并的符号加入词汇表
    
    current_vocab_freqs = merge_pair(best_pair, current_vocab_freqs)
    print("Updated corpus representation:", current_vocab_freqs)
    print("Current vocab size:", len(vocab))

print("\nFinal learned merges (in order):", learned_merges)
print("Final vocabulary (subwords):", sorted(list(vocab)))

# 文本分词 (Encoding) 示例
def tokenize_with_bpe(text, learned_merges_ordered, initial_vocab):
    # 预处理：与训练时类似，例如分割成词，添加</w>
    # 这里简化为处理单个已预处理的词（字符列表）
    # text_symbols = list(text) + ['</w>'] # 假设输入是单个词
    
    # 假设 text 是一个已经按字符分割并添加了 </w> 的词，如 "l o w </w>".split()
    word_symbols = text.split()

    # 不断应用合并规则，直到没有更多可应用的合并
    # 合并规则通常按优先级应用（例如，学习到的顺序，或按长度）
    # 此处简化为按学习顺序迭代应用（实际应用中，通常是贪婪最长匹配或按rank）
    
    # 更准确的BPE编码是贪婪地应用已学习的合并规则
    # 从高优先级（后学习到的，通常代表更长/更频繁的子词）到低优先级
    # 或者，重复扫描并应用能找到的最高优先级的合并
    
    # 一个简化的贪婪应用方法：
    # 迭代地将最高优先级的（或最先学习到的）适用合并应用于当前词的符号列表
    # 直到没有更多合并可以应用。
    
    # 以下是一个更符合BPE编码过程的简化逻辑：
    # 将文本分割成初始符号
    # word_symbols = list(text) # 如果是原始文本
    # word_symbols = text.split() # 如果是空格分隔的符号串

    # 对合并规则按某种优先级排序（例如，按学习顺序，或者按子词长度）
    # 此处我们简单地按学习顺序（learned_merges）
    
    # 这是一个迭代替换的过程，直到没有更多的合并可以进行
    # 每次迭代，我们都尝试应用所有已知的合并规则
    # 这是一个复杂的过程，因为一次合并可能创造新的合并机会
    # 一个更常见的方法是：
    # 1. 将词分解为字符。
    # 2. 迭代地找出当前词中最优先（例如，在merges列表中出现最早）的可以合并的对。
    # 3. 合并该对。
    # 4. 重复直到没有更多在merges列表中的对可以合并。
    
    # [25] 中的 encode 函数提供了一个很好的参考
    # 它迭代地在当前词的二元组中寻找在bpe_codes（即learned_merges）中出现最早的那个进行合并

    # 简化版：
    # 假设 learned_merges 是按优先级（例如，后学习的优先）排序的
    # for pair_to_merge in reversed(learned_merges): # 假设后学习的优先级高
    #     merged_token = pair_to_merge + pair_to_merge
    #     # 在 word_symbols 中查找并替换 pair_to_merge
    #     # 这需要一个循环来处理一个词中多次出现的情况
    #     # 例如： "a b a b c" 和 merge ('a','b') -> "ab ab c"
    #     # 这是一个复杂的替换逻辑，此处从略
    
    # 基于[25]的思路：
    if not word_symbols:
        return
        
    while True:
        min_rank_pair = None
        min_rank = float('inf')
        
        # 找到当前词中可以合并的、且在learned_merges中rank最小（最早学习到）的pair
        current_pairs =
        for j in range(len(word_symbols) - 1):
            current_pairs.append((word_symbols[j], word_symbols[j+1]))

        found_match_in_this_iteration = False
        for k_idx, pair_in_merges in enumerate(learned_merges): # k_idx是rank
            if pair_in_merges in current_pairs: # 如果这个学习到的合并规则适用于当前词
                # 找到第一个可以应用的学习到的合并
                # [25]
                # 我们这里用 k_idx 作为 rank
                # 找到当前词中所有二元组中，在 learned_merges 中最早出现的那个
                
                # 找到当前词中，可以应用的最早学习到的合并规则
                # 遍历词中的所有二元组
                best_pair_to_apply_now = None
                best_pair_rank = float('inf')
                
                temp_idx = 0
                while temp_idx < len(word_symbols) - 1:
                    current_word_pair = (word_symbols[temp_idx], word_symbols[temp_idx+1])
                    try:
                        rank = learned_merges.index(current_word_pair)
                        if rank < best_pair_rank:
                            best_pair_rank = rank
                            best_pair_to_apply_now = current_word_pair
                    except ValueError: # current_word_pair 不在 learned_merges 中
                        pass
                    temp_idx += 1

                if best_pair_to_apply_now is None: # 没有可以应用的合并了
                    found_match_in_this_iteration = False
                    break 
                
                # 应用这个最早学习到的、且适用于当前词的合并
                first, second = best_pair_to_apply_now
                merged_sym = first + second
                new_word_symbols =
                l = 0
                while l < len(word_symbols):
                    if l < len(word_symbols) -1 and word_symbols[l] == first and word_symbols[l+1] == second:
                        new_word_symbols.append(merged_sym)
                        l += 2
                        # 只合并一次，然后重新评估 [25]
                        # 为了简单，我们这里也只合并第一个找到的
                        # 如果需要合并所有，则需要更复杂的逻辑或多次迭代
                        new_word_symbols.extend(word_symbols[l:])
                        word_symbols = new_word_symbols
                        found_match_in_this_iteration = True
                        break # 跳出内层while，重新评估整个词
                    else:
                        new_word_symbols.append(word_symbols[l])
                        l += 1
                if found_match_in_this_iteration: # 如果在内层while中发生了合并
                    word_symbols = new_word_symbols # 更新词的符号列表
                    break # 跳出外层for (learned_merges的迭代)，重新从头开始找最早的合并
            # 如果内层循环（遍历词的二元组）没有找到可合并的，则found_match_in_this_iteration为False
            if not found_match_in_this_iteration and k_idx == len(learned_merges) -1 : # 如果遍历完所有merges都没找到匹配
                 break # 结束外层while循环


        if not found_match_in_this_iteration: # 如果一轮完整的 learned_merges 迭代下来都没有发生合并
            break # 结束主 while 循环
            
    return word_symbols


# 示例：对 "lowest</w>" 进行分词
# 假设 learned_merges 包含 ('s', 't</w>') -> 'st</w>', ('e', 'st</w>') -> 'est</w>', ('o', 'w') -> 'ow', ('l', 'ow') -> 'low'
# (顺序可能影响结果，真实的 learned_merges 来自训练)
# 假设 learned_merges (按学习顺序):
# [('e', 's'), ('es', 't</w>'), ('o', 'w'), ('l', 'ow'), ('low', 'est</w>')] # 这是一个假设的顺序
# 实际的 learned_merges 来自上面的训练过程
example_merges = [('s', 't</w>'), ('e', 'st</w>'), ('l', 'o'), ('lo', 'w'), ('low', 'er</w>')] # 假设的合并规则
# print(tokenize_with_bpe("l o w e s t </w>", example_merges, vocab))
# print(tokenize_with_bpe("h u g g i n g </w>", learned_merges, vocab))

# 注意：上面的 tokenize_with_bpe 是一个非常简化的版本，
# 真正的BPE编码器（如HuggingFace Tokenizers或SentencePiece中的）会更高效和鲁棒。
# 它通常使用已排序的合并规则（按rank）并贪婪地应用。
# 例如，tiktoken的实现是基于rank的。
BPE的流行源于其概念上的简单性和经验上的有效性，尤其是在文本压缩和处理OOV词方面 19。然而，其贪婪的本质意味着它不保证全局最优的分割 19。此外，标准BPE对预分词的依赖（例如，按空格分割）7 可能限制其形成跨越预定义边界（如空格）的词符的能力，从而可能错失对常见多词表达的合并机会。这一局限性直接推动了如BoundlessBPE等算法的出现 29。最优字节对编码问题是APX完备的 19，这意味着任何多项式时间算法（如BPE）本质上都是一种近似，这为研究更复杂或更具语言学动机的方法提供了理由。3.3. WordPieceWordPiece是另一种流行的子词分词算法，被BERT、DistilBERT、Electra等模型采用 5。3.3.1. 算法原理：基于似然的合并与最长匹配分割与BPE类似，WordPiece也从单个字符的词汇表开始，通过迭代合并符号来构建一个固定大小的词汇表 17。与BPE的关键区别在于合并标准： WordPiece并非合并最频繁出现的符号对，而是合并那些在加入词汇表后能够最大化训练数据似然值的符号对 17。词汇构建过程：初始化词汇表： 包含所有基础字符以及模型可能需要的特殊词符。训练语言模型（隐式）： 基于当前词汇表，在语料库上（隐式地）构建一个语言模型。计算合并得分： 对于当前词汇表中所有可能的相邻符号对，计算一个得分。这个得分通常基于该符号对的频率与其组成部分各自频率的某种关系，例如点互信息（PMI）或一个似然增加得分 21。一个常用的计算公式是：得分=(第一个元素的频率×第二个元素的频率)该符号对的频率​ 21。这种评分方式优先考虑那些其单个组成部分相对不那么频繁的符号对。选择与合并： 挑选得分最高的符号对进行合并，并将新合并的符号添加到词汇表中。迭代： 重复步骤2-4，直到达到预期的词汇表大小，或者似然值的增加低于某个预设阈值 30。文本分词（编码）过程：对于给定的单词，WordPiece采用贪婪的“最长匹配优先”（Maximum Matching）策略进行分割 21。它从单词的当前剩余部分开始，迭代地查找存在于已学习词汇表中的最长前缀。不处于单词开头的子词（wordpieces）通常会添加一个特殊前缀（如BERT中的“##”）来标识 8。3.3.2. 数学表述：评分函数与优化WordPiece词汇构建的核心是其合并标准。根据 21，一个符号对 (t1​,t2​) 的得分计算如下：score(t1​,t2​)=freq(t1​)×freq(t2​)freq(t1​t2​)​其中，t1​t2​ 代表合并后的新词符。算法在每次迭代中选择使此得分最大化的符号对 $(t_1^, t_2^)$进行合并：(t1)​=arg(t1​,t2​)max​score(t1​,t2​)这种评分机制与PMI相关，旨在找出那些共同出现频率相对于它们各自独立出现频率而言“出乎意料地”高的符号对。整个过程的目标是最大化在生成的词符表示下训练数据的似然。分割时的最长匹配策略：给定一个单词 W 和一个学习好的词汇表 V（通常组织成Trie树以便高效查找 33）：当 W 非空时，循环执行：a.  查找 W 的最长前缀 s，使得 s∈V（如果 s 不是词首，则可能是 ## + s′ ∈V）。b.  如果找不到这样的 s（例如，遇到未知字符序列），则将该部分视为未知（例如，分解为单个字符或使用特殊的 <UNK> 词符）。c.  将找到的 s 添加到词符列表中。d.  从 W 的开头移除 s 部分，更新 W。这种分割方法是贪婪的。文献 33 提到了 LinMaxMatch 算法，它可以将单次分词的复杂度优化到 O(n)（n 为输入词的长度）。3.3.3. Python 实现：核心逻辑以下Python代码片段概念性地展示了WordPiece分词（编码）的核心逻辑，参考了 33。词汇学习部分由于其复杂性（涉及迭代地重新评估整个语料的似然或近似得分）未在此处完整展示，但Hugging Face tokenizers 库提供了训练实现 36。文本分词 (使用最长匹配进行编码):Python# 假设 'vocab' 是一个包含已学习WordPiece词符的集合或Trie。
# 例如，来自 [35] 的词汇表示例: vocab = {"[UNK]", "the", "qu", "##ick", "br", "##own", "fox", "."}
# [34] 指出其实现使用Trie以进行高效的最长匹配。

def tokenize_word_wordpiece(word, vocabulary, unk_token="[UNK]", prefix="##"):
    """
    使用最长匹配策略将单个单词分解为WordPiece词符。
    'vocabulary' 应该是一个能快速查找成员的数据结构，如set或Trie。
    """
    if word in vocabulary: # 首先检查整个词是否在词汇表中
        return [word]

    output_tokens =
    start = 0
    while start < len(word):
        end = len(word)
        current_subword = None
        
        # 从当前位置开始，尝试匹配尽可能长的子词
        while end > start:
            sub = word[start:end]
            token_to_check = sub
            if start > 0: # 如果不是词的开头，则添加前缀进行检查
                token_to_check = prefix + sub
            
            if token_to_check in vocabulary:
                current_subword = token_to_check
                break # 找到了当前位置开始的最长匹配
            
            # 如果没有##前缀的版本在词汇表中 (只对词首部分有意义)
            if start == 0 and sub in vocabulary:
                current_subword = sub
                break

            end -= 1 # 缩短子词长度，继续尝试

        if current_subword is None: # 如果在所有长度下都没有找到匹配
            # 回退到处理单个字符，或者如果整个词都无法分解，则标记为UNK
            # 一个更鲁棒的实现会确保所有单个字符都在词汇表中（可能带##前缀）
            # 或者将无法匹配的部分标记为UNK
            # 此处简化：如果第一个字符就无法匹配，则整个词为UNK
            if start == 0:
                 output_tokens.append(unk_token)
                 return output_tokens # 无法分词
            else: # 之前已经匹配了一部分，但剩余部分无法匹配
                 # 这种情况理论上不应发生，如果词汇表构建良好且包含所有字符
                 # 为简单起见，我们假设这种情况会导致错误或剩余部分为UNK
                 remaining_part = word[start:]
                 # 简单的处理：将剩余部分视为UNK，或者按字符分解（如果字符在词汇表中）
                 # 这里我们假设如果到这里，就意味着无法继续，之前的匹配可能是错误的
                 # 一个更复杂的算法可能需要回溯
                 # 为了演示，我们简单地将剩余部分作为UNK处理，但这不完美
                 # 实际上，WordPiece会确保任何词都能被分解，最终会分解到字符级别（带##）
                 # 如果字符本身不在词汇表中，则整个词可能是UNK
                 # 此处简化：
                 # 我们应该总是能匹配到至少一个字符（如果所有字符都在词汇表中）
                 # 如果上面逻辑正确，current_subword不应该为None，除非是空字符串输入
                 # 或者词汇表不包含基础字符
                 # 假设基础字符都在词汇表中（可能带##）
                 # 如果真的没有匹配，则可能是该字符是UNK
                single_char_token = word[start]
                if start > 0:
                    single_char_token = prefix + single_char_token
                if single_char_token in vocabulary:
                    output_tokens.append(single_char_token)
                    start += 1
                    continue
                else: # 连单个字符都匹配不了
                    output_tokens.append(unk_token)
                    # 通常，如果单个字符都无法匹配，整个词可能在预处理阶段就被标记为UNK
                    # 或者WordPiece会确保所有字符都在初始词汇表中
                    # 此处跳出循环，表示分词失败或部分失败
                    break 


        output_tokens.append(current_subword)
        # 更新起始位置，注意去掉 '##' 的长度
        if current_subword.startswith(prefix) and start > 0:
            start += len(current_subword) - len(prefix)
        else:
            start += len(current_subword)
            
    return output_tokens

# 使用 Hugging Face tokenizers 库的示例 [34, 36]
# from tokenizers import BertWordPieceTokenizer
# # 训练一个新的WordPiece分词器
# tokenizer_trainer = BertWordPieceTokenizer(
#     clean_text=True,
#     handle_chinese_chars=True, # 通常对英文不重要
#     strip_accents=True,
#     lowercase=True,
# )
# files = ["path/to/your/corpus.txt"] # 训练语料文件路径列表
# tokenizer_trainer.train(
#     files,
#     vocab_size=30000,
#     min_frequency=2,
#     show_progress=True,
#     special_tokens=", "[UNK]", "", "", ""],
#     limit_alphabet=1000,
#     wordpieces_prefix="##",
# )
# tokenizer_trainer.save_model("./my_wordpiece_tokenizer")

# # 使用训练好的分词器 (或预训练的)
# # from transformers import BertTokenizer
# # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# # tokens = tokenizer.tokenize("reading a storybook!")
# # print(tokens) # ['reading', 'a', 'story', '##book', '!']
BPE纯粹基于频率的合并策略虽然简单，但有时可能产生频率高但语言学上不够内聚的子词。WordPiece的评分函数 21 通过将符号对的频率除以其各组成部分频率的乘积，试图找出那些在共现方面“大于各部分之和”的符号对。这可以被视为朝着更具语言学依据的合并策略迈出的一步，因为它优先考虑那些共现具有统计显著性而不仅仅是高频率的符号对。这可能导致生成的子词更好地与词素或有意义的词片段对齐。同时，其在分割时采用的“最长匹配优先”策略 33 也是计算语言学中词语切分的常用启发式方法。这种对合并“价值”的评估 17 使得WordPiece在构建词汇表时可能做出更精细的决策。3.4. Unigram 语言模型分词器Unigram分词器，被ALBERT、T5、XLNet等模型以及SentencePiece库所采用 13，采用了与BPE和WordPiece不同的策略。3.4.1. 算法原理：概率化分割与词汇剪枝Unigram分词器的工作方向与BPE、WordPiece相反：它从一个较大的候选词汇表开始，通过迭代剪枝，逐步将其缩减到期望的词汇表大小 4。初始词汇表： 可以由语料库中的高频子串构成，或者通过在一个非常大的词汇表规模上运行BPE或WordPiece来生成 20。词汇构建（训练）过程：初始化： 从一个包含大量候选子词的集合开始。概率估计： 根据每个子词在语料库中的频率（在初始分割后）估计其概率，例如，$P(\text{子词}) = \frac{\text{count(子词)}}{\text{total_counts}}$ 20。迭代优化（EM算法）：E步 (Expectation)： 对于训练语料中的每个词，使用当前词汇表和子词概率，通过维特比（Viterbi）算法找到最可能的子词分割序列 20。一个分割序列的概率是其组成子词概率的乘积。M步 (Maximization)： 基于E步得到的分割结果，重新估计词汇表中每个子词的概率。计算损失与剪枝： 对词汇表中的每个子词，计算如果将其移除，会导致整个语料库的似然值（通常是负对数似然的总和，即 ∑−logP(词)）增加多少。这个增加量即为该子词的“损失” 20。剪除一定比例（例如10-20%）“损失”最小的子词，即那些移除后对整体似然影响最小的子词。基础字符通常会被保留，以确保任何词都能被表示 20。重新估计剩余子词的概率。重复： 重复第3步的迭代优化过程，直到词汇表大小达到预设目标。文本分词（编码）过程：给定一个新的词，Unigram分词器会找出所有可能的、由最终词汇表中的词符组成的分割方式。然后，计算每种分割方式的概率（即组成该分割的各词符概率之乘积）。选择概率最高的那个分割序列作为最终的分词结果 20。这一过程通常也依赖维特比算法高效完成。Unigram的一个显著特点是它可以为同一个词生成多种可能的分割，并赋予它们各自的概率，这为“子词正则化”（subword regularization）等技术提供了基础 37。在训练语言模型时，可以从这些可能的分割中进行采样，而不是总是使用最优分割，这相当于一种数据增强，可以提高模型的鲁棒性。3.4.2. 数学表述：Unigram模型、维特比算法与损失函数Unigram模型概率： 对于一个词符序列 X=(x1​,x2​,…,xm​)，其在Unigram模型下的概率为：P(X)=i=1∏m​P(xi​)其中 P(xi​) 是词符 xi​ 出现的概率，通常根据其在训练语料库（经过某轮分割后）中的频率来估计 20。维特比算法进行分割： 为了找到一个词 W 的最可能分割 X∗：X∗=argX∈Segmentations(W)max​xi​∈X∏​P(xi​)维特比算法能够高效地找到这个最优路径。令 δt​(j) 为任何以词符 t 结束在位置 j 的路径的最大概率（或对数概率）。对于一个词 W=c1​c2​…cL​：令 S(i) 表示词 W[1…i]（即从第一个字符到第 i 个字符的子串）的最优分割的得分（最大对数概率）。S(i) 可以通过以下递推关系计算：S(i)=0≤j<i,使得 W[j+1…i]∈Vmax​(S(j)+logP(W[j+1…i]))其中 V 是当前词汇表，S(0)=0。通过回溯指针可以重构出最优分割路径。剪枝的损失函数： 训练的目标是最大化语料库的似然。用于剪枝的损失通常定义为：如果从词汇表 V 中移除一个词符 tj​，语料库 D=w1​,…,wN​ 的负对数似然会增加多少 20。整体损失（负对数似然）为：L(V)=−k=1∑N​freq(wk​)logP(wk​∣V)其中 P(wk​∣V) 是词 wk​ 在当前词汇表 V 下的最优分割概率，freq(wk​) 是词 wk​ 在语料中的频率。当考虑移除词符 tj​ 时，计算其移除后的损失变化量：ΔLtj​​=L(V∖tj​)−L(V)。选择 ΔLtj​​ 最小的那些词符进行剪枝。3.4.3. Python 实现：核心逻辑以下Python代码片段概念性地展示了Unigram分词器使用维特比算法进行分割的核心逻辑，参考了 20。完整的训练过程（包括EM迭代和剪枝）较为复杂，此处从略。Hugging Face tokenizers 库和 SentencePiece 库提供了Unigram的完整实现。文本分词 (使用维特比进行编码):Pythonimport math

def unigram_viterbi_segment(word, model_log_probs):
    """
    使用维特比算法为单个词找到最可能的Unigram分割。
    'model_log_probs' 是一个字典，映射: token -> log_probability。
    确保所有单个字符都在 model_log_probs 中，或者有一个统一的UNK处理。
    """
    length = len(word)
    # best_scores[i] 存储 word[:i] 的最优分割的对数概率
    best_scores = [-math.inf] * (length + 1)
    best_scores = 0.0  # 空字符串的对数概率为0
    
    # backpointers[i] 存储构成 word[:i] 最优分割的最后一个词符的起始位置
    backpointers =  * (length + 1)

    for i in range(1, length + 1):  # i 是当前子串的结束位置 (exclusive)
        for j in range(i):          # j 是当前考虑的最后一个词符的起始位置 (inclusive)
            subword = word[j:i]
            if subword in model_log_probs:
                current_score = best_scores[j] + model_log_probs[subword]
                if current_score > best_scores[i]:
                    best_scores[i] = current_score
                    backpointers[i] = j # 记录的是最后一个token的起始点
            # 如果词汇表保证覆盖所有单字符，则不需要特殊的UNK处理逻辑在这里
            # 否则，如果subword不在model_log_probs中且长度为1，可以赋予一个低的UNK概率
            # elif len(subword) == 1 and "[UNK]" in model_log_probs:
            #     current_score = best_scores[j] + model_log_probs["[UNK]"]
            #     if current_score > best_scores[i]:
            #         best_scores[i] = current_score
            #         backpointers[i] = j


    if best_scores[length] == -math.inf:
        # 如果无法分割（例如，词汇表不包含所有字符且没有UNK处理）
        # 则返回一个表示未知的列表，或按字符分割（如果字符在词汇表中）
        # 实际的Unigram模型通常会保证所有单个字符都在初始词汇表中，并且不会被删除
        # 因此总能找到一个分割（最差情况是全部分割为单字符）
        # 这里简化处理：
        if "[UNK]" in model_log_probs:
             return ["[UNK]"]
        else: # 尝试按字符分割
            tokens =
            for char_idx in range(length):
                char = word[char_idx]
                if char in model_log_probs:
                    tokens.append(char)
                else: # 连单字符都不在词汇表中，这是个问题
                    return ["[UNK]"] # 最终回退
            return tokens


    # 回溯找到最优路径
    tokens =
    current_end_pos = length
    while current_end_pos > 0:
        prev_end_pos = backpointers[current_end_pos]
        tokens.append(word[prev_end_pos:current_end_pos])
        current_end_pos = prev_end_pos
    
    return tokens[::-1] # 反转得到正确的顺序

# 示例来自 [20] (概率需要预先计算并转换为对数概率)
# 假设的词频 [20]
# freqs = {"h": 15, "u": 36, "g": 20, "hu": 15, "ug": 20, 
#          "p": 17, "pu": 17, "n": 16, "un": 16, "b": 4, 
#          "bu": 4, "s": 5, "hug": 15, "gs": 5, "ugs": 5}
# total_freq_sum = sum(freqs.values()) # 在[20]的例子中是 210

# model_probs_example = {token: freq / total_freq_sum for token, freq in freqs.items()}
# model_log_probs_example = {token: math.log(prob) for token, prob in model_probs_example.items() if prob > 0}

# # 确保所有单个字符都在词汇表中，即使频率为0（赋予极小概率）
# all_chars_in_corpus = set("".join(corpus_counts.keys())) # corpus_counts from BPE example
# for char_ in all_chars_in_corpus:
#     if char_ not in model_log_probs_example:
#         model_log_probs_example[char_] = -math.inf # 或者一个非常小的对数概率

# print(f"Tokens for 'unhug': {unigram_viterbi_segment('unhug', model_log_probs_example)}")
# # 预期输出 [20]: ['un', 'hug']

# print(f"Tokens for 'pug': {unigram_viterbi_segment('pug', model_log_probs_example)}")
# # 预期输出 (基于 [20] 的例子，可能是 ['p', 'ug'] 或 ['pu', 'g'])
# # [20]/[20] 提到 "pug" 会被分为 ['p', 'ug'] 或 ['pu', 'g']
# # 取决于哪个先遇到，因为它们的概率相同：
# # P(["p", "ug"]) = P("p")*P("ug") = (17/210)*(20/210) = 0.002267
# # P(["pu", "g"]) = P("pu")*P("g") = (17/210)*(20/210) = 0.002267
# # P(["p", "u", "g"]) = (17/210)*(36/210)*(20/210) = 0.00132
# # 所以 ['p', 'ug'] 或 ['pu', 'g'] 更优
Unigram算法与BPE的纯粹基于频率的贪婪计数或WordPiece的成对似然评分不同，它在一个更整体的概率框架内运作。其目标是找到一个能最大化整个语料库（在Unigram语言模型假设下）似然的词汇表 20。使用维特比算法进行分割，确保了在该模型下能得到概率最高的切分方式。而其剪枝步骤，基于移除那些对整体语料库似然影响最小的词符，是一种有原则地减小词汇量的方法，同时试图保留从统计建模角度看最重要的词符。这种概率框架也自然地允许为同一个词产生多种带有相应概率的切分方案，从而为诸如子词正则化（subword regularization）等技术提供了可能 37，这有助于提升模型的鲁棒性。Unigram对整个词的分割进行建模，而不是像BPE或WordPiece那样聚焦于局部符号对的合并，这使其能够从全局角度优化词汇表。表1: 主流子词分词算法对比特性字节对编码 (BPE)WordPieceUnigram 语言模型核心机制数据压缩驱动；迭代合并最常见符号对基于似然；迭代合并能最大化训练数据似然的符号对概率模型驱动；从大词汇表开始，迭代剪枝以优化语料库整体似然词汇构建自底向上：从字符开始，重复合并最频繁的相邻符号对，直到达到目标词汇量 19。自底向上：从字符开始，重复合并使模型（隐式）似然增加最大的符号对，直到达到目标词汇量 21。自顶向下：从一个大的初始词汇表（如所有子串或BPE生成的大词汇表）开始，使用EM算法迭代移除对整体语料库似然贡献最小的词符，直到达到目标词汇量 20。分割策略贪婪应用学习到的合并规则（通常按rank或最长匹配），将文本分割成词汇表中最长的已知子词 23。贪婪最长匹配：迭代查找词中在词汇表内存在的最长前缀进行分割 21。维特比算法：找到给定词在当前词汇表和词符概率下的最可能的分割序列 20。关键数学概念符号对频率计数；贪婪优化 19。点互信息 (PMI) 或似然增加评分函数：freq(t1​)freq(t2​)freq(t1​t2​)​ 21。Unigram 语言模型概率 P(X)=∏P(xi​)；最大似然估计；维特比算法；EM算法进行剪枝 20。OOV 处理通常通过字节级表示或将未知字符分解到初始字符集来处理，目标是无OOV 11。通过将未知词分解为已知子词（最终到字符级别，带##前缀）或映射到<UNK>词符处理 21。通过将未知词分解为已知子词（最终到字符级别）或映射到<UNK>词符处理；可以为分割分配概率 20。典型应用模型GPT系列, RoBERTa, Llama, BART 5。BERT, DistilBERT, Electra 5。ALBERT, T5, XLNet, SentencePiece (可配置为Unigram) 20。主要优点简单高效，对OOV有良好覆盖，压缩效果好。倾向于生成语言学上更合理的子词，通过似然优化合并。概率框架，提供带概率的多种分割方式，剪枝有原则。主要缺点贪婪算法可能非最优；预分词可能限制合并 19。训练可能比BPE复杂；分割时仍是贪婪最长匹配。训练过程（EM迭代，维特比）计算成本较高。4. 先进及前沿分词方法在BPE、WordPiece和Unigram等基础子词分词算法之上，研究者们不断探索新的方法以克服现有技术的局限性，追求更优的语言学特性、更高的效率或更强的模型性能。4.1. 突破预分词限制：BoundlessBPE标准BPE及其变体通常依赖于预分词步骤（例如，按空格分割文本），这会阻止跨越这些预设边界的合并操作 7。这种做法可能导致词符分布向常见的、完整长度的预分词单元（如单个词）严重倾斜，从而限制了扩大词汇量所能带来的益处，因为新增的词符出现频率可能较低 29。BoundlessBPE 29 是一种改进的BPE算法，它放宽了预分词边界的严格约束。核心思想： 允许相邻的、本身已经是单个词符的预分词单元合并成更大的“超词”（superword）词符。例如，预分词单元 " a" 和 " the"（假设它们在词汇表中已是单个词符）可以合并成新的超词 " of the"。这些超词不一定具有严格的语义内聚性 29。机制： 只有当相邻的两个预分词单元都已经被词汇表表示为单个独立词符时，才允许它们之间进行合并 39。目标： 实现更均匀的词符分布，并提高文本的压缩效率（据称可使每词符字节数增加约20%）29。预分词虽然旨在简化主分词算法（如BPE）在处理大规模、多样化语料时的复杂性 7，但它本身也可能成为一个瓶颈。通过施加硬性边界，预分词限制了寻找最优词符的搜索空间。如果像“of the”这样的常见短语因为基于空格的预分割而无法合并，分词器就错失了获得更好压缩效果和可能更具语义意义的单元的机会。BoundlessBPE直接挑战了这一假设，表明对预分词边界采取更灵活的处理方式可以带来进一步的改进。文献 7 均讨论了僵硬预分词的负面影响，尤其是在处理某些语言或追求更高压缩率时。当一个预分词单元（如一个词）在早期训练中就变成一个单独的词符后，它在后续合并中的作用实际上就结束了，这限制了形成跨词边界的更长、更有效词符的可能性 29。4.2. 优化最小词符序列：PathPiecePathPiece 22 算法的目标是对于给定的词汇表，将文本分割成尽可能少的词符数量。核心假设： 更少的词符数量能带来更好的下游任务性能 22。机制：分割（Segmentation）： PathPiece通过在有向无环图（DAG）中寻找最短路径来实现文本分割。在该图中，每个字节位置是一个节点，如果两个节点之间的字节片段是词汇表中的一个有效词符，则它们之间存在一条边 22。词汇构建（Vocabulary Construction）： 采用自顶向下的过程，从一个大的初始词汇表开始，通过迭代地删除那些对最小化语料库总词符数（Corpus Token Count, CTC）贡献最小的词符批次，来启发式地构建最终词汇表 22。PathPiece对最小化词符数量的追求 22，直接体现了“高效压缩是有效分词的良好代理指标”这一思想 7。通过迫使算法寻找最紧凑的表示，它鼓励在可能的情况下形成更长、更具包容性的词符。这间接地推动分词器识别更大、语义上更连贯的单元，而不是不必要地将文本分解。PathPiece与Unigram之间的联系（例如，PathPiece可以视为一种特殊的加权Unigram，其中所有词符概率相等 22）表明，这种压缩目标可以通过有原则的方式实现。4.3. 融合上下文信息：SaGeSaGe (Contextual Tokenizer) 22 旨在构建一个更适合语言模型应用的词汇表，其方法是选择那些在语料库中出现于尽可能清晰的上下文集合中的词符 39。机制： SaGe通过“使用skip-gram目标的消融损失”（ablation loss via a skipgram objective）将上下文信息融入词汇构建过程 22。它采用自顶向下的方式，从一个初始词汇表开始进行剪枝。目标： 与BPE或PathPiece相比，SaGe倾向于产生一组“更独特”的词符 22，这些词符因其上下文相关性而被选中。BPE和WordPiece主要依赖子串的统计共现性，Unigram则考虑词符的概率。SaGe 22 更进一步，明确地尝试对词符出现的上下文进行建模。其类似skip-gram的目标函数表明，它学习到的词符表示能够预测其邻近词符。这代表了一种趋势，即分词不仅要追求统计上的高频或高概率，还要追求在其典型语言环境中的语义内聚性。这可能产生出对依赖上下文理解的下游任务更直接有用的词符。4.4. 引入随机性：StochasTok 与子词正则化传统的确定性分词器对于相同的输入总是产生相同的分词结果。这种确定性有时可能会掩盖单词精细的内部结构，或限制模型的鲁棒性 37。随机分词（Stochastic Tokenization）允许对同一输入产生多种可能的分词结果 37。子词正则化 (Subword Regularization, Kudo, 2018) 37：作为Unigram的扩展，它在训练语言模型时，根据学习到的Unigram模型概率从多种可能的子词分割中进行采样。这充当了一种数据增强形式，可以提高模型的鲁棒性。BPE-dropout (Provilkov et al., 2020) 37：通过在BPE编码（分割）过程中随机省略一些已学习到的合并操作来引入随机性。StochasTok 37：允许大型语言模型（LLM）“透视”词符内部，其方法是以一定的概率随机地将现有词符（来自基础分词器的输出）分解为其在词汇表中等价的、更小的词符对。这个过程可以迭代进行。步骤 37：使用基础分词器（如BPE或Unigram）对文本进行初始分词，得到一个token_ids列表。迭代地执行“扩展”步骤：随机选择一个词符，如果可能，则将其分裂成词汇表中存在的一对等价的更小词符（例如，如果"unhappiness"是一个词符，且"un"和"happiness"也在词汇表中，则可以分裂）。这个过程重复 $p \cdot \text{len(token_ids)}$ 次，其中 p 是一个超参数。潜在益处： 随机分词可以提高模型在需要理解子词结构的任务（如某些语言游戏、数学问题解答）上的性能，并且有可能将子词理解能力注入到现有的预训练模型中 37。确定性分词产生固定的、离散的表示。而像子词正则化和StochasTok这样的随机方法 37 则引入了可变性。这种可变性在模型训练期间可以起到正则化器的作用，迫使模型不过度依赖单一、特定的分词结果，从而对微小的输入变化更具鲁棒性。此外，像StochasTok这样通过随机分解词符的方法，有效地允许模型探索其学习到的词符的内部结构。这有助于模型更好地理解子词语义和组合性，这些信息可能被固定、不透明的词符ID所掩盖。这与分词可能掩盖精细结构 37 以及模型需要理解组合性的观点相符。4.5. 神经分词器 (简述)一个新兴的趋势是使用神经网络来执行分词任务 39。示例： BLTs (Byte-Level Transformers for Tokenization) 39 使用Transformer架构在字节级别上学习分词。潜力： 神经分词器有望实现更具适应性和上下文感知的分词，但其“黑箱”特性可能导致调试困难 39。神经分词器 39 代表了将分词过程转变为端到端NLP流水线中另一个可学习组件的趋势。这与深度学习领域用学习组件替代手工特征或模块的更广泛趋势相一致。其潜在优势在于能够产生一个与下游模型和任务最佳协同适应的分词器。然而，正如 39 所指出的，这是以牺牲可解释性和可调试性为代价的。神经分词器的“黑箱”性质使得理解为何做出某些分词决策变得更加困难，这对于错误分析和确保公平性可能构成问题。5. 大规模语言模型 (LLM)时代的Tokenization在LLM时代，分词不仅是预处理步骤，其选择和设计对模型的性能、效率、偏见乃至安全性都有着深远影响。5.1. LLM流水线中预分词的关键作用预分词（Pre-tokenization）是在主要子词算法（如BPE）应用之前，将输入文本分解为更小、更易于管理的块的过程 7。常见的预分词策略包括基于空格和标点符号的分割，或使用正则表达式 7。例如，GPT-2推广了使用大型正则表达式进行预分词的做法 7。影响： 预分词步骤实际上规定了从这些预分词块中能够学习到的词符的最大可能长度 7。它会阻止词符跨越这些预设的边界 22。预分词的挑战：词符分布偏差： 可能导致词符分布偏向常见的、完整长度的预分词单元，从而限制了更大词汇量带来的益处 29。复杂脚本处理不当： 对于具有复杂书写系统（如泰米尔语、印地语）的语言，以英语为中心的预分词器可能会不必要地切分文本，导致需要更大的上下文窗口，并造成不公平的表示 7。例如，对于泰米尔语的“வணக்கம்”（你好），GPT-2的预分词器可能会将其切分为多个部分，而mBERT等模型的预分词器则倾向于将其保留为单个预分词单元 7。预分词是管理在大型多样化语料库上训练主分词器（如BPE）复杂性的一种实用步骤 7，它简化了输入到子词算法的数据。然而，正如 29 和 7 清晰展示的那样，这些预定义的分割充当了硬性约束。如果这些约束与文本的语言特性（尤其对于非英语语言或专业领域）不能很好地对齐，它们就会阻碍最优子词词符的形成。这会导致效率低下（序列更长，成本更高 9）和潜在的偏见 7。因此，预分词器的选择并非次要细节，而是一个具有重大下游影响的关键设计决策。一旦一个预分词块（例如一个词）在分词器训练的早期阶段成为一个单独的词符，它在后续合并过程中的作用就基本结束了，这限制了形成跨越原预分词边界的、可能更优的词符的机会 29。5.2. 主流LLM的分词策略不同的大型语言模型家族采用了不同的分词算法和预分词策略。GPT系列 (GPT-2, GPT-3, GPT-4):主要使用BPE算法 5。GPT-2推广了基于正则表达式的预分词方法 7。GPT-4和Llama 3也采用类似的预分词策略 7。GPT-2的词汇表示例大小为50,257 23。如前所述，其预分词策略在处理复杂脚本语言时存在挑战 7。Llama系列 (Llama, Llama 2, Llama 3):基于BPE 5。同样采用类似GPT模型的基于正则表达式的预分词 7。在处理复杂脚本语言时也面临与GPT系列相似的预分词挑战 7。BERT:使用WordPiece算法 5。预分词通常包括空白分割和标点处理，续尾子词使用“##”前缀 21。多语言BERT (mBERT) 的预分词器在处理复杂脚本方面优于GPT风格的正则表达式预分词器 7。T5 (及 Flan-T5, mT5):使用SentencePiece 7，它可以配置为使用BPE或Unigram算法（T5变体通常使用Unigram）。SentencePiece将输入视为原始字节流，并能学习处理词符内部的空格，这可能减少了对严格预分词的依赖 31。FLAN-T5和mT5的预分词器能较好地处理复杂脚本 7。ByT5, CANINE:这些是字节级分词器，将文本作为字节序列处理 7。它们避免了为字符/子词预定义词汇表的问题，但可能导致非常长的序列。基础LLM的分词器选择在模型训练完成后就被“固化”了 16。模型的嵌入层是为由其特定分词器生成的特定词符ID学习的。为一个预训练模型更换分词器通常需要大规模的重新训练或微调，因为输入的表示方式发生了根本性改变。这为现有模型的分词技术创新设置了很高的门槛 39。这也意味着，原始分词器中固有的任何缺陷或偏见（如词汇覆盖不足、对某些语言结构处理不当 5）会伴随模型整个生命周期并在其应用中持续存在，除非得到明确解决。分词器的词汇表和相关的嵌入矩阵是与训练数据和模型内在绑定的 16，从业者通常默认使用来自Hugging Face等库的标准分词器 41。表2: 主流LLM的分词策略概览LLM 家族具体模型示例主要分词算法关键预分词特征典型词汇量来源GPTGPT-2, GPT-3, GPT-4BPE基于大型正则表达式的分割~50k (GPT-2)7LlamaLlama 2, Llama 3BPE基于大型正则表达式的分割~32k-128k+5BERTBERT-base, mBERTWordPiece空白分割, 标点处理, "##"前缀~30k (BERT-base)5T5 / Flan-T5T5-base, Flan-T5SentencePiece (Unigram)将输入视为原始流，较少依赖严格预分词；可处理内部空格~32k7ByT5 / CANINEByT5字节级 (Byte-level)无传统意义上的预分词，直接处理UTF-8字节序列256 (字节)75.3. 分词对LLM性能、效率、偏见及专业语言处理的影响分词决策对LLM的各个方面都有显著影响：性能： 不正确或次优的分词会阻碍LLM对输入的精确理解，从而导致不理想的输出 5。例如，将一个重要的术语错误地分割成无意义的片段，会使模型难以把握其真实含义。效率与成本： 字符级分词或对罕见/复杂词的不良子词分割会导致更长的词符序列。这不仅增加了计算负荷和处理时间，还直接推高了API调用成本，尤其是在大规模应用场景下 3。偏见与公平性： 分词器可能从训练数据中习得偏见，影响对来自代表性不足社群或特定语言的文本的分割方式 9。这可能导致对文化术语、名称或小众领域语言的分割显得笨拙或不准确 9。特别是预分词策略对复杂脚本语言的处理，可能导致“不公平的表示”，使得这些语言需要更长的上下文窗口才能表达相同信息 7。专业语言（如金融、医疗）： 如果专业术语没有被分词器很好地表示（例如，常见的医学术语“preauthorization”被分割成“pre”、“author”、“ization”），模型可能会难以准确理解其含义，从而影响在这些专业领域的应用准确性和可靠性 9。OOV/罕见词处理： 尽管子词模型能够将罕见词分解为已知的子词片段，但这仍然可能影响准确性，并因为产生更多词符而增加序列长度和处理成本 9。多语言模型： 由于不同语言具有迥异的语法结构和词汇特点，多语言环境下的分词更具挑战性。一个主要基于英语训练的分词器可能无法有效处理其他文字系统（如阿拉伯文、芬兰文）的文本，尤其是那些具有丰富形态变化的语言（词根相同但词形多变）9。对抗性攻击： 一些分词策略容易受到恶意操纵。攻击者可以精心构造输入，使得文本在分词后呈现给模型的是一种无害或误导性的形式，从而绕过安全检测或导致模型错误分类，尽管其原始语义意图可能并未改变（例如TokenBreak攻击 4）。研究表明，Unigram等策略可能对此类攻击具有更好的防御性 4。分词的设计和训练中所做的选择，其影响远超纯粹的技术效率层面。正如 9 和 7 所强调的，分词直接决定了来自不同语言社群或专业领域的文本能否得到良好表示，从而影响了公平性和偏见问题。对专业术语的低效分词 9 会限制LLM在医疗、金融等关键行业的实用性。而像TokenBreak这样的漏洞 4 表明，分词本身也可能成为一个攻击向量。因此，分词技术的研究与发展不仅要考虑算法的优雅性或压缩效率，还必须审视其更广泛的伦理、社会和安全影响。分词并非仅仅是一个技术步骤，它更是LLM能力及其社会影响的一个重要调节器。6. 评估分词方法评估分词方法的有效性至关重要，通常可以从内部指标（intrinsic）和外部指标（extrinsic）两个维度进行。6.1. 内部评估指标内部评估指标基于分词器本身或分词后文本的属性来评价其质量，无需训练完整的下游模型。压缩率 / 每词符字节数 (Bytes Per Token, BPT)： 衡量分词器压缩文本的程度。通常认为，更高的压缩率（即相同文本产生更少的词符，或每个词符能表示更多的字节）更好 7。BPT的计算方式是 UTF-8编码的字节数 / 生成的词符数 7。有研究表明文本压缩与语言模型性能之间存在显著关系，认为压缩是一个可靠的内在质量指标 7。然而，也有研究对其普适可靠性提出疑问，并建议结合其他指标 42。词汇量大小 (Vocabulary Size)： 分词器使用的独特词符数量 3。需要在覆盖度和模型大小之间取得平衡。子词多样性/丰富度 (Subword Fertility)： 平均每个（原始）单词被分割成的词符数量。理想值为1.0，表示大多数单词未被切分或被切分得恰到好处。较低的值通常更好，意味着单词没有被过度分割 7。续词比例 (Proportion of Continued Words)： 被分割成多个词符的单词所占的百分比。较低的值表示更好的性能，即更少的词被切断 7。归一化序列长度 (Normalized Sequence Length, NSL)： 将分词器产生的平均词符序列长度与一个基线分词器的结果进行比较 7。分词均等性 (Tokenization Parity, TP)： 评估一种语言相对于另一种语言的分词情况（例如，处理相同语义内容时，泰米尔语词符与英语词符的比例）。用于评估多语言环境下的公平性 7。未登录词率 (Out-of-Vocabulary, OOV Rate)： 对于词级别分词器，指词汇表中未包含的单词百分比。对于能覆盖所有字符的子词分词器，此比率理论上应为零。认知合理性指标 (Cognitive Plausibility Metrics)：将分词器输出与人类在词汇判断任务中的阅读时间、反应准确率等数据进行关联分析 44。“组块性”（Chunkability，定义为字符数/词符数的倒数，即每词符字符数）可作为衡量处理难度的指标，较高的组块性可能对应更容易的处理、更高的准确率和更短的阅读时间 44。尽管存在多种内部评估指标 7，但目前尚无任何单一指标被普遍认为是跨所有任务和语言的最佳下游性能预测器。例如，42 指出文本压缩作为通用指标的可靠性受到质疑，并提出了新的相关指标。而 44 则引入了认知合理性作为评估维度。这种多样性表明，“好的分词”是一个多层面的概念。一个分词器可能在压缩方面表现出色，但产生的子词在语言学上不合理，反之亦然。指标的选择往往取决于具体目标（例如，效率、语言学可解释性、公平性）。对通用内在指标的探索仍在进行中，例如 tokenizers_intrinsic_benchmark 39 这样的基准测试项目，就在积极研究内在指标与下游任务性能之间的相关性，这本身就说明了这是一个活跃的研究领域。6.2. 外部评估：对下游NLP任务的影响外部评估通过衡量分词器对特定下游NLP任务（如机器翻译、文本分类、问答系统）性能的影响来评价其好坏 3。这被认为是检验分词器在特定应用中实用性的最终标准。然而，外部评估的计算成本非常高昂，因为它需要训练和评估完整的语言模型 39。这种高成本也构成了分词技术创新的一个障碍，因为验证新分词算法的改进需要大量的计算资源。研究 41 探讨了分词器训练数据规模对下游性能的影响，而 42 则研究了在小型模型上的评估结果是否能可靠地预测其在大型模型上的影响。基于模型在特定任务（如机器翻译 3）上的表现来评估分词器，可以说是衡量其在该任务中“好坏”的最权威方法。然而，这种方法的代价高昂且耗时。此外，一个对机器翻译最优的分词器，未必对情感分析或代码生成等其他任务也是最优的。这种任务特定性意味着，尽管外部评估很有价值，但若不进行跨多个任务的广泛测试，它们很难导向通用的“最佳”分词器。这也反过来强调了寻找可靠且计算成本较低的内部评估指标的重要性，这些内部指标可以作为外部评估的有效代理。6.3. 对比分析与基准测试考量在比较不同的分词器时，需要考虑多个因素，包括：所用算法的类型、词汇表的大小、训练数据的特性（领域、规模、语言）、以及预分词步骤等 7。研究 41 表明，对于分词器的训练数据规模，当达到一定程度后（例如，对于英语BPE/Unigram/WordPiece，大约在150GB左右），其带来的收益会递减。一些研究论文，如 22，提供了不同分词器在各种指标上的对比表格。理想的基准测试应覆盖多种语言和任务，例如 43 的工作就侧重于印度语言。一个分词方法的性能与其实现细节（例如，预分词规则 7）和训练配置（例如，词汇量大小、训练语料 41）紧密相关。简单地比较一个库中的BPE与另一个库中在不同数据、不同词汇量下训练的WordPiece，可能无法得出关于算法本身的有效结论。严格的基准测试要求仔细控制这些变量，以分离出核心算法差异所带来的影响。像 22（PathPiece论文）和 7（预分词影响研究）这样的研究，正试图进行这种受控比较，以确保评估的公平性和结果的可靠性。因此，进行“同等条件”下的比较虽然困难，但对于获得有意义的评估结果至关重要。表3: Tokenization评估指标概览指标名称描述衡量内容简要公式 (若适用)优点缺点典型用途BPT/压缩率每词符平均表示的字节数，或原始大小与分词后大小的比率。压缩效率，信息密度。UTF-8字节数 / 词符数直观，易计算，与某些下游性能相关 7。可能不完全反映语义切分质量；对某些语言可能不公平 42。通用分词器效率评估。子词多样性/丰富度平均每个原始词被分割成的子词数量。分割的粒度，是否过度分割。总子词数 / 总原词数反映词的完整性保留程度 43。理想值可能因语言和任务而异。评估子词切分的合理性。续词比例被切分成多个子词的原始词的百分比。词边界的保留情况。(被切分词数 / 总原词数) * 100%直观显示多少词被“打断” 43。与子词丰富度相关。评估词完整性的保留。归一化序列长度 (NSL)相对于一个基准分词器，当前分词器产生的平均序列长度。相对压缩效率。当前分词器平均序列长度 / 基准分词器平均序列长度提供相对比较基准 43。依赖于基准分词器的选择。不同分词器效率的横向比较。分词均等性 (TP)一种语言相对于另一种语言的分词效率（如每英语词符对应的目标语言词符数）。多语言场景下的公平性和资源需求。(例如) 目标语言词符数 / 英语词符数 (针对平行语料)量化多语言处理中的潜在不均衡 7。需要平行或可比语料。评估多语言模型的公平性和效率。OOV 率词汇表中未包含的词的百分比（主要针对词级分词器）。词汇覆盖度。(OOV词数 / 总词数) * 100%直接反映对新词的处理能力。对子词分词器意义不大（理论上应为0）。传统词级分词器评估。认知合理性 (如组块性)分词结果与人类语言处理模式（如阅读时间、词汇判断准确率）的符合程度。组块性可定义为每词符字符数。分词的心理学现实性。(组块性) 字符数 / 词符数提供新的评估维度，探索分词是否符合人类认知 44。认知数据获取难；关联性可能复杂，受多种因素影响。研究分词与人类语言理解的关系。下游任务性能分词器在具体NLP任务（如翻译、分类）上训练出的模型的最终表现。分词对实际应用的最终影响。(任务特定指标，如BLEU, F1)最能反映分词的实用价值 3。计算成本高，耗时长；结果可能任务特定，难以泛化 39。验证分词策略对特定应用的有效性。7. 当前挑战与未来研究方向尽管分词技术取得了显著进展，但仍面临诸多挑战，同时也展现出广阔的研究前景 1。7.1. 持续存在的障碍真正的语义分词： 当前主流的子词分词方法主要依赖统计模式而非深层语义理解。实现能够稳定地将文本切分为基本语义单元的“真正”语义分词，仍然是一个长远目标。对噪声和对抗性攻击的鲁棒性： 现有分词器在处理拼写错误、非正式语言以及蓄意构造的对抗性输入（如TokenBreak攻击 4）时，其鲁棒性仍有待提高。高效处理演化语言： 语言是动态变化的，新词、俚语和用法层出不穷。如何在不频繁重训整个模型的前提下，使分词器能优雅地适应这些语言演化，是一个重要挑战。跨模态对齐与一致性： 对于多模态模型（例如结合文本和图像的模型），确保不同模态的词符化（tokenization）既有意义又相互一致，是一个复杂的问题 12。码本坍塌与利用率 (Codebook Collapse and Utilization)： 在学习词汇表（码本）时，需要确保词汇得到充分利用，避免出现大量冗余或极少使用的词符，这会影响模型的效率和表达能力 12。固有权衡： 分词设计中始终存在一些难以两全的权衡，例如在压缩率与信息保真度之间、在模型的理解能力与生成能力之间取得平衡 12。与基础模型的集成： 大型基础模型一旦选定并训练好其分词器，后续的创新和更新就变得非常困难，因为分词器与其表示层紧密耦合，“固化”在模型中 12。当前大多数分词器在训练完成后都使用一个固定的词汇表。然而，语言本身是鲜活且不断演化的实体：新词汇不断涌现，词义发生漂移，俚语和网络用语更是日新月异。一个固定的词汇表在面对这些新情况时，不可避免地会遇到困难，往往只能将新词分解成意义不甚明确的子词片段，或者表现出类似OOV（未登录词）的行为。这揭示了当前主流分词方案的静态性与语言动态性之间的根本矛盾。未来的研究需要解决分词器如何能更平滑地适应语言的这种演变，而无需对庞大的LLM进行代价高昂的完全重训练。7.2. 新兴趋势与未来方向针对上述挑战，分词领域的研究正朝着更智能、更灵活、更高效的方向发展 12。自适应与动态分词 (Adaptive and Dynamic Tokenization)： 开发能够根据上下文、领域甚至随时间动态调整其行为的分词器 12。这可能包括动态管理键值缓存（KV cache）的稀疏性，以在推理时提高效率 45。神经分词器 (Neural Tokenizers)： 如前所述 39，利用神经网络学习分词过程，有望实现更强的上下文感知能力和端到端的优化。高效训练与推理 (Efficient Training and Inference)： 研发更快速的算法来训练分词器和执行分词操作，尤其针对超大规模数据集和实时应用场景 12。架构创新 (Architectural Innovations)： 探索超越当前BPE、WordPiece、Unigram等范式的新型分词器架构 12。更优的词符重要性度量 (Better Token Importance Metrics)： 开发更鲁棒、无偏的词符重要性评分机制，可能利用元学习等方法，以最小监督捕获词符对下游任务的效用，从而实现跨任务和领域的自适应剪枝或选择 45。建设性词符压缩 (Constructive Token Compression)： 从纯粹的删除式剪枝（eliminative pruning）转向能够将空间或语义上相似的词符合并成紧凑摘要向量的策略 45。缓解位置偏见 (Mitigating Position Bias)： 在多模态语言模型中，基于注意力的剪枝方法可能导致保留的词符集中在图像的特定区域。未来的方法应通过在保留的词符中强制结构均匀性来保持空间多样性，以提高视觉任务的鲁棒性 45。跨模态引导剪枝 (Cross-Modal Guided Pruning)： 在多模态语言模型中，词符剪枝决策应由模态间的依赖关系引导，而不是对每个模态独立进行。例如，文本引导的视觉词符剪枝可以改善模态间对齐 45。端到端稀疏化 (End-to-End Sparsification)： 分词和词符削减应同时考虑LLM的预填充（prefill）阶段和解码（decoding）阶段，包括动态管理KV缓存的稀疏性并选择性地更新生成的词符，从而在整个推理过程中保持效率增益 45。许多未来的研究方向，如自适应分词、神经分词器、跨模态引导剪枝等 12，都预示着分词技术正从一个固定的、独立的预处理步骤，演变为与语言模型的学习过程和整体架构更紧密集成的组件。这表明未来可能会出现分词器与模型协同设计的趋势，即分词策略能够受到模型内部状态或特定任务需求的影响。这种深度融合有望带来更高效、更有效的文本表示，但同时也增加了系统的复杂性。这与分词器作为连接原始数据和模型内部表示的关键接口的定位 12，以及独立于模型训练进行分词器创新所面临的困难 39 等观察相吻合。8. 结论8.1. 分词技术演进与核心算法范式回顾本报告系统性地回顾了英文分词技术的发展历程，从早期基于简单规则（如空格、标点）和正则表达式的方法，到具有里程碑意义的宾州树库（PTB）标准，再到现代大规模语言模型中占据主导地位的子词分词算法——字节对编码（BPE）、WordPiece和Unigram语言模型。每一种范式的演进都旨在克服前代方法的局限性，并在词汇量控制、未登录词（OOV）处理、计算效率以及语言信息表示等多个维度上寻求更优的平衡。基于规则的方法提供了直接的控制和可解释性，但在泛化能力和维护成本上存在不足。PTB标准则为学术研究提供了一致性基准，但也揭示了“词”边界定义的复杂性。子词分词的出现，通过在词级和字符级之间找到中间地带，显著改善了对大规模、多样化文本的处理能力，尤其是在处理罕见词、形态变化以及平衡词汇表大小与表示能力方面取得了巨大成功。BPE以其简单高效的频率合并机制成为主流；WordPiece通过引入基于似然的合并标准，试图生成更具语言学意义的子词单元；而Unigram则在一个概率框架下，通过迭代剪枝和基于维特比算法的分割，提供了更灵活和有原则的词汇构建与文本切分方式。随后，报告探讨了针对这些基础算法局限性而提出的高级与前沿方法，如旨在突破预分词限制的BoundlessBPE，追求最小词符序列的PathPiece，融合上下文信息的SaGe，引入随机性的StochasTok，以及初露端倪的神经分词器。这些新方法反映了分词技术正朝着更精细、更智能、更适应特定需求的方向发展。8.2. 关于分词策略选择与发展的总结性思考分词作为自然语言处理流程的基石，其重要性不言而喻。然而，本报告的分析表明，不存在一种“放之四海而皆准”的通用分词解决方案。特定分词策略的选择和开发，必须综合考量多种因素，包括目标语言的特性、处理语料的领域和规模、下游任务的具体需求、可用的计算资源，以及面临的特定挑战（例如，专业术语的覆盖、多语言环境的适应性、对噪声和对抗性攻击的鲁棒性等）。当前，预分词步骤对LLM分词结果的影响日益受到重视，尤其是在处理非英语语言或特定领域文本时，其设计不当可能成为性能瓶颈。同时，分词技术对模型公平性、效率和成本的直接影响，也要求研究者和开发者在设计分词方案时，必须超越单纯的技术指标，审视其潜在的社会伦理意涵。评估分词方法的有效性本身也是一个复杂的问题，需要结合内部指标（如压缩率、子词多样性、认知合理性）和外部指标（下游任务性能）进行综合判断。尽管外部评估被视为最终的试金石，但其高昂的成本促使研究者不断寻求更可靠且经济的内部评估手段。展望未来，分词技术的研究将继续聚焦于如何更紧密地与语言模型的学习过程和架构设计相结合，以实现更自适应、上下文感知和端到端的优化。对动态语言现象的高效处理、对多模态信息的一致性表示，以及在保证性能的同时提升分词过程的可解释性和鲁棒性，将是未来研究的关键方向。随着AI技术的不断进步，对更先进、更智能分词方法的需求也将持续驱动该领域的创新与发展。参考文献(本报告引用了多个来源，包括arXiv论文、nlpprogress.com等网站以及相关技术文档。具体引用信息已在正文中通过格式标出。)附录 (可选)(本报告中提供了核心算法的Python概念性实现代码片段。完整的、可运行的BPE、WordPiece和Unigram实现通常依赖于成熟的NLP库，如Hugging Face tokenizers、SentencePiece等，或特定研究者开源的代码库。)